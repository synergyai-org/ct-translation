{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ruamel.yaml import YAML\n",
    "\n",
    "yaml = YAML()\n",
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "\n",
    "logger.add(\"debugging.log\")\n",
    "\n",
    "class rcc_3d_dataset(Dataset):\n",
    "    def __init__(self, list_id, config):\n",
    "        self.config = config\n",
    "        self.sub_sample_freq = 5\n",
    "\n",
    "        with h5py.File(config['path_data'], 'r', swmr=True) as f:\n",
    "            start_indexes = []\n",
    "            accumulated_count = 0\n",
    "            new_list_id = []\n",
    "            for id in list_id:\n",
    "                try: \n",
    "                    shape_pre = f[id]['image']['precontrast_r']['Axial'].shape\n",
    "                    shape_post = f[id]['image']['post_50sec']['Axial'].shape\n",
    "                    shape_late = f[id]['image']['post_5min_r']['Axial'].shape\n",
    "                    shape_mask = f[id]['segmentation']['post_50sec']['Axial'].shape\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "                assert(shape_pre == shape_post == shape_late == shape_mask)\n",
    "                start_indexes.append(accumulated_count)\n",
    "                accumulated_count += (shape_pre[0])//self.sub_sample_freq\n",
    "                new_list_id.append(id)\n",
    "\n",
    "        self.list_id = new_list_id\n",
    "        self.start_indexes = start_indexes ##\n",
    "        self.tot_count = accumulated_count ##\n",
    "        self.path_data = config['path_data']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tot_count\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        id_index = np.digitize(idx, self.start_indexes)-1\n",
    "\n",
    "        # random slice\n",
    "        slice_index = (idx - self.start_indexes[id_index]) * self.sub_sample_freq + np.random.randint(0, 5)\n",
    "        # random left or right\n",
    "        orientation = np.random.randint(0, 2) \n",
    "\n",
    "        image_x = h5py.File(self.path_data, 'r', swmr=True)[self.list_id[id_index]]['image']['precontrast_r']['Axial'][slice_index:slice_index+1, 167:423, orientation*256:(orientation+1)*256]\n",
    "        image_y = h5py.File(self.path_data, 'r', swmr=True)[self.list_id[id_index]]['image']['post_50sec']['Axial'][slice_index:slice_index+1, 167:423, orientation*256:(orientation+1)*256]\n",
    "\n",
    "        image_x = (image_x/2000).astype(np.float32)\n",
    "        image_y = (image_y/2000).astype(np.float32)\n",
    "\n",
    "        image_x = np.clip(image_x, 0, 1)\n",
    "        image_y = np.clip(image_y, 0, 1)    \n",
    "\n",
    "        # -1, 1로 넣어줘야하는 듯\n",
    "        image_x = image_x * 2 -1\n",
    "        image_y = image_y * 2 -1\n",
    "        return torch.from_numpy(image_x), torch.from_numpy(image_y)\n",
    "\n",
    "def struct_dataset(phase,):\n",
    "    yaml = YAML()\n",
    "    with open('/home/synergyai/jth/rcc-classification-research/experiments/config/session_240621.yaml') as f:\n",
    "        config = yaml.load(f)\n",
    "\n",
    "    split_dict = {}\n",
    "    with open(config['path_split'],'r') as f_split:\n",
    "        split = f_split.read().splitlines()\n",
    "        for i, split_name in enumerate(['train', 'valid', 'test']):\n",
    "            string = split[i]\n",
    "            string = string.replace('[','').replace(']','').replace(\"'\", \"\")\n",
    "            id_list = string.split(', ')\n",
    "            split_dict[split_name] = id_list\n",
    "    \n",
    "    if phase == 'train':\n",
    "        list_id = split_dict['train']\n",
    "    elif phase == 'val':\n",
    "        list_id = split_dict['valid']\n",
    "\n",
    "    return rcc_3d_dataset(list_id, config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = struct_dataset('train')\n",
    "val_ds = struct_dataset('val')\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=config['train']['batch_size'], shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=config['train']['batch_size'], shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, y0 = train_ds.__getitem__(1)\n",
    "#(b, c, h, w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#seed=1024, resume=False, image_size=256, num_channels=2, centered=True, \n",
    "# use_geometric=False, beta_min=0.1, beta_max=20.0, num_channels_dae=64, \n",
    "# n_mlp=3, ch_mult=[1, 1, 2, 2, 4, 4], num_res_blocks=2, attn_resolutions=(16,), \n",
    "# dropout=0.0, resamp_with_conv=True, conditional=True, fir=True, fir_kernel=[1, 3, 3, 1], \n",
    "# skip_rescale=True, resblock_type='biggan', progressive='none', progressive_input='residual', \n",
    "# progressive_combine='sum', embedding_type='positional', fourier_scale=16.0, not_use_tanh=False, \n",
    "# exp='exp_syndiff', input_path='/home/synergyai/jth/ct-translation-research/input', \n",
    "# output_path='/home/synergyai/jth/ct-translation-research/output', nz=100, num_timesteps=4, \n",
    "# z_emb_dim=256, t_emb_dim=256, batch_size=1, num_epoch=500, ngf=64, lr_g=0.00016, lr_d=0.0001, \n",
    "# beta1=0.5, beta2=0.9, no_lr_decay=False, use_ema=True, ema_decay=0.999, r1_gamma=1.0, \n",
    "# lazy_reg=10, save_content=True, save_content_every=10, save_ckpt_every=10, lambda_l1_loss=0.5, \n",
    "# num_proc_node=1, num_process_per_node=1, node_rank=0, local_rank=0, master_address='127.0.0.1', \n",
    "# contrast1='T1', contrast2='T2', port_num='6021', world_size=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128, 16])\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/changzy00/pytorch-attention/blob/master/vision_transformers/ViT.py\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, qkv_bias=False, attn_drop=0, proj_drop=0):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.attn_drop = nn.Dropout(attn_drop) if attn_drop else nn.Identity()\n",
    "        self.proj_drop = nn.Dropout(proj_drop) if proj_drop else nn.Identity()\n",
    "        self.norm = nn.GroupNorm(num_groups = min(dim//4, 32), num_channels = dim, eps = 1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch, sequence, channel\n",
    "        B, N, C = x.shape\n",
    "        x = self.norm(x.transpose(1,2)).transpose(1,2)\n",
    "        #x = self.norm(x)\n",
    "        # batch, sequence, (q/k/v), head, head_dim\n",
    "        # -> (q/k/v), batch, head, sequence, head_dim\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # q,k,v : batch, head, sequence, head_dim\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        # batch, head, sequence_q, sequence_k -> batch, head, sequence_k, sequence_q / scale by sqrt(dim)\n",
    "        attn = (q @ k.transpose(-1, -2)) * self.scale\n",
    "        # batch, head, sequence_k, sequence_q / softmax along sequence_q\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        # (batch, head, sequence_k, sequence_q) * (batch, head, sequence, head_dim)\n",
    "        # -> (batch, head, sequence_k, head_dim) -> (batch, sequence_k, head, head_dim) -> (batch, sequence_k, dim)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "attn = Attention(dim = 16, num_heads=2)\n",
    "x = torch.randn(10, 128, 16)\n",
    "x = attn(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TORCH_CUDA_ARCH_LIST'] = '8.6'\n",
    "from upfirdn2d import upfirdn2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upsample_conv_2d(x, w, k=None, factor=2, gain=1):\n",
    "  \"\"\"Fused `upsample_2d()` followed by `tf.nn.conv2d()`.\n",
    "\n",
    "     Padding is performed only once at the beginning, not between the\n",
    "     operations.\n",
    "     The fused op is considerably more efficient than performing the same\n",
    "     calculation\n",
    "     using standard TensorFlow ops. It supports gradients of arbitrary order.\n",
    "     Args:\n",
    "       x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n",
    "         C]`.\n",
    "       w:            Weight tensor of the shape `[filterH, filterW, inChannels,\n",
    "         outChannels]`. Grouped convolution can be performed by `inChannels =\n",
    "         x.shape[0] // numGroups`.\n",
    "       k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n",
    "         (separable). The default is `[1] * factor`, which corresponds to\n",
    "         nearest-neighbor upsampling.\n",
    "       factor:       Integer upsampling factor (default: 2).\n",
    "       gain:         Scaling factor for signal magnitude (default: 1.0).\n",
    "\n",
    "     Returns:\n",
    "       Tensor of the shape `[N, C, H * factor, W * factor]` or\n",
    "       `[N, H * factor, W * factor, C]`, and same datatype as `x`.\n",
    "  \"\"\"\n",
    "\n",
    "  assert isinstance(factor, int) and factor >= 1\n",
    "\n",
    "  # Check weight shape.\n",
    "  assert len(w.shape) == 4\n",
    "  convH = w.shape[2]\n",
    "  convW = w.shape[3]\n",
    "  inC = w.shape[1]\n",
    "  outC = w.shape[0]\n",
    "\n",
    "  assert convW == convH\n",
    "\n",
    "  # Setup filter kernel.\n",
    "  if k is None:\n",
    "    k = [1] * factor\n",
    "  k = _setup_kernel(k) * (gain * (factor ** 2))\n",
    "  p = (k.shape[0] - factor) - (convW - 1)\n",
    "\n",
    "  stride = (factor, factor)\n",
    "\n",
    "  # Determine data dimensions.\n",
    "  stride = [1, 1, factor, factor]\n",
    "  output_shape = ((_shape(x, 2) - 1) * factor + convH, (_shape(x, 3) - 1) * factor + convW)\n",
    "  output_padding = (output_shape[0] - (_shape(x, 2) - 1) * stride[0] - convH,\n",
    "                    output_shape[1] - (_shape(x, 3) - 1) * stride[1] - convW)\n",
    "  assert output_padding[0] >= 0 and output_padding[1] >= 0\n",
    "  num_groups = _shape(x, 1) // inC\n",
    "\n",
    "  # Transpose weights.\n",
    "  w = torch.reshape(w, (num_groups, -1, inC, convH, convW))\n",
    "  w = w[..., ::-1, ::-1].permute(0, 2, 1, 3, 4)\n",
    "  w = torch.reshape(w, (num_groups * inC, -1, convH, convW))\n",
    "\n",
    "  x = F.conv_transpose2d(x, w, stride=stride, output_padding=output_padding, padding=0)\n",
    "\n",
    "  return upfirdn2d(x, torch.tensor(k, device=x.device),\n",
    "                   pad=((p + 1) // 2 + factor - 1, p // 2 + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernalized_filter(k=[1, 3, 3, 1], gain = 1, factor = 2):\n",
    "    k = np.asarray(k, dtype=np.float32)\n",
    "    if k.ndim == 1:\n",
    "        k = np.outer(k, k)\n",
    "    k /= np.sum(k)\n",
    "    return k*(gain*(factor**2))\n",
    "\n",
    "# custom upsample with fir filtration\n",
    "class conv_upscale_with_fir_sampling(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size = 3, fir_params = (1,3,3,1), scale_factor = 2):\n",
    "        super().__init__()\n",
    "        self.kernalized_filter = get_kernalized_filter(fir_params, factor=scale_factor)\n",
    "        padding_value = (self.kernalized_filter.shape[0] - scale_factor) // (kernel_size-1)\n",
    "        stride = [1,1,scale_factor,scale_factor]\n",
    "\n",
    "        self.weight = nn.Parameter(torch.zeros(out_ch, in_ch, kernel_size, kernel_size))\n",
    "        \n",
    "\n",
    "        self.conv = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, channel, height, width)\n",
    "        x.shape()\n",
    "\n",
    "\n",
    "\n",
    "        self.Conv2d_0 = up_or_down_sampling.Conv2d(in_ch, out_ch,\n",
    "                                                 kernel=3, up=True,\n",
    "                                                 resample_kernel=fir_kernel,\n",
    "                                                 use_bias=True,\n",
    "                                                 kernel_init=default_init())\n",
    "        \n",
    "\n",
    "    def __init__(self, dim, scale_factor=2, mode='bilinear'):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=scale_factor, mode=mode)\n",
    "        self.norm = nn.GroupNorm(num_groups = min(dim//4, 32), num_channels = dim, eps = 1e-6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.norm(x.transpose(1,2)).transpose(1,2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attn block, upsample, downsample, pyramid downsample, resnetblockbiggan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MultiheadAttention.forward() missing 2 required positional arguments: 'key' and 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m16\u001b[39m,)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# (B,C,patch_H, patch_W)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jth/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jth/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: MultiheadAttention.forward() missing 2 required positional arguments: 'key' and 'value'"
     ]
    }
   ],
   "source": [
    "\n",
    "# (B,C,H,W)\n",
    "x = torch.rand(10,128,16,16,)\n",
    "# (B,C,patch_H, patch_W)\n",
    "attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        return F.relu(out)\n",
    "    \n",
    "class NCSN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NCSN, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResNetBlock(256, 256) for _ in range(6)]\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResNetGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetGenerator, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResNetBlock(256, 256) for _ in range(6)]\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_image = torch.randn(1, 1, 256, 256)  # Batch size of 1, single channel 256x256 image\n",
    "gen = ResNetGenerator()\n",
    "output_image = gen(input_image)\n",
    "print(output_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.nograd()\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x, y in tqdm(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss/len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, in_axis=1, out_axis=0, dtype=torch.float32, device='cpu', variance=1.0):\n",
    "    def _compute_fans(shape, in_axis=1, out_axis=0):\n",
    "        receptive_field_size = np.prod(shape) / shape[in_axis] / shape[out_axis]\n",
    "        fan_in = shape[in_axis] * receptive_field_size\n",
    "        fan_out = shape[out_axis] * receptive_field_size\n",
    "        return fan_in, fan_out\n",
    "\n",
    "    # Compute fan_in and fan_out\n",
    "    fan_in, fan_out = _compute_fans(shape, in_axis, out_axis)\n",
    "\n",
    "    # Use fan_in for the denominator in the scaling factor\n",
    "    denominator = fan_in\n",
    "\n",
    "    # Calculate the scaling factor\n",
    "    scaling_factor = np.sqrt(3 * variance / denominator)\n",
    "\n",
    "    # Generate the weights\n",
    "    weights = (torch.rand(*shape, dtype=dtype, device=device) * 2. - 1.) * scaling_factor\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _compute_fans(shape, in_axis=1, out_axis=0):\n",
    "# fan_in, fan_out = _compute_fans(shape, in_axis, out_axis)\n",
    "# receptive_field_size = np.prod(shape) / shape[in_axis] / shape[out_axis]\n",
    "# fan_in = shape[in_axis] * receptive_field_size\n",
    "# fan_out = shape[out_axis] * receptive_field_size\n",
    "# denominator = fan_in\n",
    "# (torch.rand(*shape, dtype=dtype, device=device) * 2. - 1.) * np.sqrt(3 * variance)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
